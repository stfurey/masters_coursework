{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d10iJj2hYNl"
      },
      "source": [
        "# Homework 4 - MLPs and Regularization\n",
        "\n",
        "In this assignment, you will analyze the effect of regularization techniques (L2 regularization and dropout) on the performance of MLP models built using PyTorch. Let's get started.\n",
        "\n",
        "## Task 1: Load Dataset\n",
        "\n",
        "We will use the Breast Cancer dataset from scikit-learn. Let's load and preprocess this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9VbdvzhBhYNn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8c9824d8-fc85-4ed0-a288-e37340eb00a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(398, 30) (171, 30)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X, y = dataset.data, dataset.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Example data size\n",
        "print(X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P2PnUYUhYNp"
      },
      "source": [
        "## Task 2: Implement a Simple MLP with PyTorch\n",
        "\n",
        "This week we will implement MLP with two layers (one hidden layer) using PyTorch. PyTorch's implementation already supports training with and without L2 regularization and dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-bJVrOZ_hYNp"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.0):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5poM77K4hYNp"
      },
      "source": [
        "## Task 3: Train the MLP with PyTorch\n",
        "\n",
        "Implement the training loop for the MLP with options for L2 regularization and dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kmLbh7KDhYNp"
      },
      "outputs": [],
      "source": [
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=25):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.float(), labels.float().view(-1, 1)  # Convert data to the appropriate type and shape\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize the parameters\n",
        "            running_loss += loss.item()\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6HM9qclhYNq"
      },
      "source": [
        "## Your Tasks\n",
        "\n",
        "1. Complete the DataLoader creation: Use TensorDataset and DataLoader to create loaders for the training and test datasets.\n",
        "2. Instantiate the MLP model: Define the model using appropriate dimensions for input, hidden, and output layers.\n",
        "3. Define the loss function and optimizer: Use BCELoss for binary classification and Adam optimizer with L2 regularization.\n",
        "4. Train the model with and without dropout: Modify the dropout_rate and observe the performance differences.\n",
        "5. Evaluate the model: Implement an evaluation function to calculate accuracy on the test dataset.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "343mAghjhYNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1a2746c7-dc29-41e1-b212-a47d99cc8a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=0\n",
            "Accuracy: 98.25%\n",
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=0.0001\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=0.01\n",
            "Accuracy: 98.83%\n",
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=0.1\n",
            "Accuracy: 98.83%\n",
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=1\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.0 and L2 Regularization=10\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=0\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=0.0001\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=0.01\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=0.1\n",
            "Accuracy: 98.83%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=1\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.2 and L2 Regularization=10\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=0\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=0.0001\n",
            "Accuracy: 98.25%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=0.01\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=0.1\n",
            "Accuracy: 98.25%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=1\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.5 and L2 Regularization=10\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=0\n",
            "Accuracy: 99.42%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=0.0001\n",
            "Accuracy: 98.83%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=0.01\n",
            "Accuracy: 98.25%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=0.1\n",
            "Accuracy: 98.25%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=1\n",
            "Accuracy: 63.16%\n",
            "\n",
            "Training with Dropout=0.8 and L2 Regularization=10\n",
            "Accuracy: 63.16%\n"
          ]
        }
      ],
      "source": [
        "# Your code starts here\n",
        "\n",
        "# 1. Create DataLoaders (0.5 pt)\n",
        "# Use TensorDataset to convert the training and test data into a dataset format\n",
        "# Use DataLoader to create batches of data for training and testing\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 2. Instantiate the model (0.5 pt)\n",
        "# Define the input dimension based on the number of features in the dataset\n",
        "# Define the hidden dimension and output dimension (binary classification, so output_dim = 1)\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 50\n",
        "output_dim = 1\n",
        "dropout_rate = 0.50\n",
        "model = MLP(input_dim, hidden_dim, output_dim, dropout_rate)\n",
        "\n",
        "# 3. Define loss function and optimizer (0.5 pt)\n",
        "# Use BCELoss (binary cross entropy) from torch.nn for binary classification\n",
        "# Use Adam optimizer (instead of a fixed learning rate) and include weight_decay parameter for L2 regularization\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "# 4. Train the model (0.5 pt)\n",
        "# Train the model using the train_model function defined above\n",
        "num_epochs = 25\n",
        "trained_model = train_model(model, criterion, optimizer, train_loader, num_epochs)\n",
        "\n",
        "# 5. Evaluate the model (1 pt)\n",
        "# Define an evaluation function to compute the accuracy of the model on the test data\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.float(), labels.float().view(-1, 1)  # Convert data to the appropriate type and shape\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            predicted = (outputs > 0.5).float()  # Apply a threshold to get binary predictions (0 or 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# evaluate_model(trained_model, test_loader)\n",
        "\n",
        "\n",
        "\n",
        "# 6. Change the above code to experiment with different dropout rates in the range from 0 to 1,\n",
        "# also experiment with different L2 regularization parameters (weight_decay), e.g., 0, 0.0001, 0.01, 0.1, 1, 10, etc. ,\n",
        "# and summarize your findings. (1 pt)\n",
        "\n",
        "\n",
        "dropout_rates = [0.0, 0.2, 0.5, 0.8]\n",
        "l2_values = [0, 0.0001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "for dr in dropout_rates:\n",
        "    for l2 in l2_values:\n",
        "        print(f\"\\nTraining with Dropout={dr} and L2 Regularization={l2}\")\n",
        "        model = MLP(input_dim, hidden_dim, output_dim, dropout_rate=dr)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2)\n",
        "\n",
        "        trained_model = train_model(model, criterion, optimizer, train_loader, num_epochs)\n",
        "        evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Findings\n",
        "\n",
        "Most models had accuracy over 98%, when the weight decay was 1 or 10 then the model performed significantly worse. The high values led to underfitting. The rest of the models were overfitted, most likely because we are using a toy dataset."
      ],
      "metadata": {
        "id": "hIkdTzIhLUMt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}