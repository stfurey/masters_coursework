{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN4u_-VSfh2l"
      },
      "source": [
        "# Homework 10: Coding GPT-2 #\n",
        "\n",
        "In this assignment, we will graduate to state-of-the-art methods in Language Modeling and Generative AI. Most of you have heard of ChatGPT. ChatGPT is powered by the GPT architecture under the hood (GPT-4 at this moment). The underlying idea behind the GPT family, introduced in [this paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), is quite intuitive: given the preceding $k - 1$ words (the context), we want to predict the current word. Formally, given an unsupervised corpus of tokens $U = {u_1, ..., u_n}$, our goal is to maximize the\n",
        "following log-likelihood\n",
        "\n",
        "$$L(U) = \\sum_i log P(u_i \\mid u_{i−k}, · · · , u_{i−1}; θ)$$\n",
        "\n",
        "where $P(u_i \\mid u_{i−k}, · · · , u_{i−1}; θ)$ (the probability of any word given all preceding words) is parameterized by a transformer-based neural network. The main novelty of the GPT series of models from OpenAI is their very large scale in terms of both the size of the model and dataset that the model was being trained on. As a point of reference, GPT-3 is powered by a model with 175 billion parameters and was trained on 300 billion tokens of text, costing roughly 10 million dollars to train. Obviously we will not\n",
        "be able to train such a model from scratch in this assignment; instead, we will use the weights provided by OpenAI for GPT-2 (GPT-3 is too big to fit in normal computer memory, especially for Colab). With that said, we need to build the correct framework (i.e. implement the correct model) so we can load\n",
        "the weights into our built framework. In this question, you will essentially implement the Transformer (there are libraries, for example PyTorch, that have already implemented the transformer for use as a built-in\n",
        "function. We will instead implement it from scratch for a better understanding).\n",
        "\n",
        "You can read an excellent [blog about transformers here](https://jalammar.github.io/illustrated-transformer/), and [about GPT-2 here](https://jalammar.github.io/illustrated-gpt2/). You will:\n",
        "\n",
        "* implement GPT-2 from scratch\n",
        "* load the pre-trained weights from OpenAI for our implemented GPT-2 (this part is taken care of for you)\n",
        "* generate fun text\n",
        "\n",
        "Your tasks are described at the end of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PURkH2QrJ-Iu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This block provides the encoding funciton that GPT2 uses to encode tokens.\n",
        "This part is a bit beyond our scope for this assingment so DO NOT alter\n",
        "this code block!\n",
        "\n",
        "Byte pair encoding utilities.\n",
        "Copied from: https://github.com/openai/gpt-2/blob/master/src/encoder.py.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import regex as re\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a significant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvhwLreVKb6x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This block provides code to download the pre-trained weights from OpenAI.\n",
        "GPT2 comes with 4 different sizes with the number of weights being either\n",
        "\"124M\", \"355M\", \"774M\", or \"1558M\".\n",
        "\n",
        "Having the software engineering skill to load complicated model weights\n",
        "can be cruical, but for this assignment you will just use this code block to\n",
        "do that. We highly encourage you to read through the code and understand\n",
        "what is going on.\n",
        "\n",
        "DO NOT alter this code block.\n",
        "\"\"\"\n",
        "\n",
        "def download_gpt2_files(model_size, model_dir):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size=\"124M\", models_dir='./'):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHWoSnWyK43Z"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is the code block for implementing GPT2.\n",
        "GPT2 consists EXCLUSIVELY of transformer decoders where the number of such\n",
        "decoders is a hyperparameter (that is provided by the downloaded parameters\n",
        "from OpenAI.)\n",
        "\n",
        "You will need to fill in all the ### Your code here in this code block.\n",
        "You essentially will be implementing multi-head self-attention.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    # normalize x to have mean=0 and var=1 over last axis\n",
        "    # BE CAREFUL: when calculating the standard deviation, be sure to add\n",
        "    #             eps to the variance to avoid 0 variance. I.e.\n",
        "    #             sd = square_root(var+eps)\n",
        "\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    var = np.var(x, axis=-1, keepdims=True)\n",
        "\n",
        "    x = (x - mean) / np.sqrt(var + eps)\n",
        "\n",
        "    return g * x + b  # scale and offset with gamma/beta params\n",
        "\n",
        "\n",
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    # Implement this simple linear layer, i.e. simply implement\n",
        "    # output = x*w + b\n",
        "    # NOTE: \"*\" denotes matrix multiplication\n",
        "\n",
        "    return np.dot(x, w) + b\n",
        "\n",
        "\n",
        "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "    # This function performs the self-attention step based on the\n",
        "    # ---the query: q\n",
        "    # ---the key: k\n",
        "    # ---the value: v\n",
        "    # ---the mask: mask\n",
        "    # We need mask as each token, for decoding purpose, can only pay attention\n",
        "    # to the tokens that come before it, i.e. we can't look into the future!\n",
        "\n",
        "    scores = np.dot(q, k.T)\n",
        "    scores += mask\n",
        "\n",
        "    attention_weights = softmax(scores)\n",
        "\n",
        "    output = np.dot(attention_weights, v)\n",
        "\n",
        "    return output\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    # we project x into a space of dimension 3*n_embd, so we need to split x\n",
        "    # into 3 matrices, each of which of dimension [n_seq, n_embd], for\n",
        "    # q, k, and v\n",
        "    # Hint: use the np.split function, so qkv should be a list\n",
        "\n",
        "    qkv = np.split(x, 3, axis=-1)\n",
        "    # q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "    # split into heads\n",
        "    # Recall that the transformer deploys multihead attention\n",
        "    # So we need to further split EACH q, k, v into n_embd/n_head matrices.\n",
        "    # So the result should be a list of lists\n",
        "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    qkv_heads = [np.reshape(part, (n_head, part.shape[0], part.shape[1] // n_head)) for part in qkv]\n",
        "\n",
        "    # q_heads = np.reshape(q, (n_head, q[0], q[1] // n_head))  # [n_head, n_seq, n_embd/n_head]\n",
        "    # k_heads = np.reshape(k, (n_head, k[0], k[1] // n_head))  # [n_head, n_seq, n_embd/n_head]\n",
        "    # v_heads = np.reshape(v, (n_head, v[0], v[1] // n_head))  # [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    # the mask should be of size [n_seq, n_seq]\n",
        "\n",
        "    causal_mask = np.tril(np.ones((x.shape[0], x.shape[0])))  # [n_seq, n_seq]\n",
        "\n",
        "    # perform attention over each head\n",
        "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    out_heads = []\n",
        "    for i in range(n_head):\n",
        "        out_heads.append(attention(qkv_heads[0][i], qkv_heads[1][i], qkv_heads[2][i], causal_mask))\n",
        "\n",
        "    # merge results from different heads\n",
        "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    x = np.concatenate(out_heads, axis=-1)\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # multi-head causal self-attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
        "\n",
        "\n",
        "def temporal_sampling(logits, tau):\n",
        "    # Apply temperature scaling to logits\n",
        "    scaled_logits = logits / tau\n",
        "\n",
        "    # Softmax function to compute probabilities\n",
        "    probs = np.exp(scaled_logits - np.max(scaled_logits))\n",
        "    probs /= np.sum(probs)\n",
        "\n",
        "    # Sample an index based on the probability distribution\n",
        "    sampled_token = np.random.choice(len(probs), p=probs)\n",
        "\n",
        "    return sampled_token\n",
        "\n",
        "\n",
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # print(inputs)\n",
        "    # print(params)\n",
        "    # print(n_head)\n",
        "    # print(n_tokens_to_generate)\n",
        "\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "\n",
        "        # print(logits)\n",
        "\n",
        "        # Choose the current token from logits. Note what the shape of the logit\n",
        "        # variable is (hint: you only want to use the logits for the token that\n",
        "        # we are predicting at this step, and ignore the previous logits)\n",
        "        # ALSO, MAKE SURE once you predict the current token, you append the id of\n",
        "        # predicted token to inputs to update inputs.\n",
        "\n",
        "        # There are also different stratgies of predicting tokens givne logits.\n",
        "        # Here, just implement the greedy strategy, i.e. choose the index\n",
        "        # that has the largest logit.\n",
        "\n",
        "        last_input_token = logits[-1, :]\n",
        "        predicted_token = np.argmax(last_input_token)\n",
        "        # predicted_token = temporal_sampling(last_input_token, 100)\n",
        "\n",
        "        inputs = np.append(inputs, predicted_token)\n",
        "\n",
        "    return inputs[-n_tokens_to_generate:]  # only return generated ids\n",
        "\n",
        "# \"124M\", \"355M\", \"774M\", \"1558M\"\n",
        "def main(prompt, n_tokens_to_generate = 40, model_size = \"1558M\", models_dir = \"gpt2_models\"):\n",
        "\n",
        "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
        "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
        "\n",
        "    # encode the input string using the BPE tokenizer\n",
        "    input_ids = encoder.encode(prompt)\n",
        "\n",
        "    # make sure we are not surpassing the max sequence length of our model\n",
        "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "    # generate output ids\n",
        "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "    # decode the ids back into a string\n",
        "    output_text = encoder.decode(output_ids)\n",
        "\n",
        "    return output_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmGXPmW6hBWX",
        "outputId": "97288c56-4f2b-4c19-f9f4-25aceaf2569b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 3.66Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 2.42Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 3.54Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Gb [10:19, 10.1Mb/s]                                   \n",
            "Fetching model.ckpt.index: 21.0kb [00:00, 434kb/s]                                                  \n",
            "Fetching model.ckpt.meta: 1.84Mb [00:00, 3.26Mb/s]                                                  \n",
            "Fetching vocab.bpe: 457kb [00:00, 1.28Mb/s]                                                         \n",
            "generating: 100%|██████████| 40/40 [10:31<00:00, 15.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\n",
            ":\n",
            ". and \" in, in- a.\n",
            " and-a Aalsam on,\n",
            ",'-50,?2-ro.s (L for \"are,\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVokImLZt-hH",
        "outputId": "08329a8a-803e-4603-e319-9c5a4aea5fc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generating: 100%|██████████| 40/40 [11:03<00:00, 16.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " sw B its I ( should-.\n",
            ", and home as a the up [ Ds in, and to the [\n",
            " on (be-\n",
            " and the [-and-t : \"\n"
          ]
        }
      ],
      "source": [
        "prompt = \"One day a golden retriever sees another golden retriever, they\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDcsoJ0bno_h"
      },
      "source": [
        "#Your Tasks: #\n",
        "\n",
        "\n",
        "1.   Complete all the <### Your Code Here> blocks in the cells above. (1.5 pt)\n",
        "2.   Show the generated text given the provided prompt in the previous cell. (0.25 pt)\n",
        "3.   Currently, you implemented a greedy approach for decoding. Is there any randomness in the greedy approach? I.e. given a trained (fixed) model, if we run the\n",
        "model multiple times given the same prompt, will we get different generated text? (0.25 pt)\n",
        "\n",
        "No, with the greedy approach we will never generate different text.\n",
        "\n",
        "4.   Using the greedy approach, generate a text of 40 tokens with the 124M model based on the following prompt:\n",
        "\n",
        "          One day a golden retriever sees another golden retriever, they\n",
        "\n",
        "Do you see any problem with the generated text? What do you think causes the problem? (0.5 pt)\n",
        "5. *Temporal sampling*: Next, we will implement a slightly more interesting sampling technique that allows us to control how creative\n",
        "the generated text will be. Temporal sampling is quite straightforward: it integrates a temperature parameter into the softmax operation. Given the logits $l = {l_1, · · · , l_{|V |}}$, a temporal softmax (operating\n",
        "on each li) takes the form of\n",
        "\n",
        "$$ s_i = \\frac{\\exp (l_i/τ )} {\\sum_{k \\in 1\\ldots |V|} exp (l_k/τ )}$$\n",
        "\n",
        "where $τ$ is a temperature parameter. We can then sample the index from a multinomidal distribution with parameters $s = {s_1, · · · , s_{|V |}}$.\n",
        "\n",
        "*   Show that when $τ → 0$, temporal sampling is equivalent to the greedy approach.\n",
        "\n",
        "*   Show that when $τ → ∞$, temporal sampling is equivalent to sampling an index from the uniform distribution. Therefore, when $τ → ∞$, what do you think the quality of the generated text will be?\n",
        "Therefore, think of $τ$ as a knob on how creative the generated text will be.\n",
        "*   Implement the temporal sampling. For the prompt\n",
        "\n",
        "           Alan Turing theorized that computers would one day become\n",
        "\n",
        "try three different $τ$ : $0.01$, $1$, and $100$ to generate three 100-token texts. Show your generations here. (1 pt)\n",
        "\n",
        "6. You can see 124M is the smallest model. Experiment with some of the bigger models and whatever prompts you like. Comparing the generated text from the different sized models on the same prompt, what conclusion can you draw? (0.5 pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ANSWERS"
      ],
      "metadata": {
        "id": "xKrOjMSTSNvw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLFLSxlztl05"
      },
      "source": [
        "1. See code blocks.\n",
        "2. - the the. when 's are ( the or no.-) the the- theque,the,-'s-, the-, will are\n",
        "3. No, with the greedy approach the model will never generate different text.\n",
        "4. them,— M, Phoenix the\" an ( the and and to the W [. -' it the- ( (\n",
        " and,( also was or the and and and- in-.\n",
        "5. See responses for different temperatures.\n",
        "\n",
        "0.01: is/-..: N\n",
        "- the the. when 's are ( the or no.-) the the- theque,the,-'s-, the-, will are\n",
        "\n",
        "1: juutes- feud, and W so the for,.....,ai)- fault ' or things to outside minded is), a about to distress: you misplaced\n",
        " let change see often\n",
        "\n",
        "100: archaeocity Such slippedoliberal Thai year CAMquist Shipsassadicals shop Blades Creed nudity Lamador Douglas policemenFocus shovelgreat forestryB Compan Eliane forgivenVersion田etheus312...? Boot OFFIC detract curb ver ...\n",
        "\n",
        "\n",
        "6. Models with more parameters were not much better than the 124M parameter model. None of the models produced coherent outputs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}